# -*- coding: utf-8 -*-
"""vision_practice.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uiy-oXDez9ClBZiIXDIT3TH25N5CXMQu
"""



import os
from unsloth import FastVisionModel
import torch
from datasets import load_dataset
from transformers import TextStreamer
from unsloth import is_bf16_supported
from unsloth.trainer import UnslothVisionDataCollator
from trl import SFTTrainer, SFTConfig

# 1. Load the model

model, tokenizer = FastVisionModel.from_pretrained(
    "Bllossom/llama-3.2-Korean-Bllossom-AICA-5B",
    load_in_4bit = True,
    use_gradient_checkpointing = "unsloth",
)

model = FastVisionModel.get_peft_model(
    model,
    finetune_vision_layers     = True,
    finetune_language_layers   = True,
    finetune_attention_modules = True,
    finetune_mlp_modules      = True,
    r = 16,
    lora_alpha = 16,
    lora_dropout = 0,
    bias = "none",
    random_state = 3407,
    use_rslora = False,
    loftq_config = None,
)



# 2. Load the dataset

pre_dataset = load_dataset('hateslopacademy/otpensource_dataset', split='train')

import json

def transform_data(data):
    """
    ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ 'output', 'instruction', 'image_url'ë¡œ ë³€í™˜.
    - instruction: "ì´ ì˜·ì€ ì–´ë–¤ ì˜·ì¸ì§€ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
    - output: big_categoryì™€ product_nameì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
    - image_url: ê¸°ì¡´ image_url ê°’ ìœ ì§€
    """
    # ë³€í™˜ ì‘ì—…
    output_data = {
        ("category" if key == "sub_category" else key): value
        for key, value in data.items()
        if key not in ["big_category", "product_name", "image_url"]
    }

    transformed = {
        "instruction" : """
ë‹¹ì‹ ì€ JSON í˜•ì‹ ë°ì´í„°ë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ AIì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ ì˜·ì— ëŒ€í•œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ ì•„ë˜ì˜ í…œí”Œë¦¿ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.


### ì¶œë ¥ í˜•ì‹ ###
{
    "category": "ì˜· ì¢…ë¥˜ (ì˜ˆ: ë¯¼ì†Œë§¤ í‹°ì…”ì¸ , ì²­ë°”ì§€, í›„ë“œ)",
    "gender": "ì°©ìš©ì ì„±ë³„ (ì˜ˆ: ë‚¨, ì—¬, ì •ë³´ ì—†ìŒ)",
    "season": "ê³„ì ˆ ì •ë³´ (ì˜ˆ: SS, FW, ì‚¬ê³„ì ˆ, ì •ë³´ ì—†ìŒ)",
    "color": "ìƒ‰ìƒ (ì˜ˆ: í™”ì´íŠ¸, ë¸”ë™)",
    "material": "ì†Œì¬ (ì˜ˆ: ìš¸, í´ë¦¬, ì •ë³´ ì—†ìŒ)",
    "feature": "íŠ¹ì§• (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´, ì˜ˆ: ìŠ¬ë¦¬ë¸Œë¦¬ìŠ¤, ë°˜íŒ”, ê¸´íŒ”)"
}

### ì…ë ¥ ì´ë¯¸ì§€ ###
[ì•„ë˜ì— ì œê³µëœ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³  JSON ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.]
"""
,
        "output": json.dumps(output_data, ensure_ascii=False),  # JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
        "image_url": data["image_url"]
    }

    return transformed

# ë°ì´í„°ì…‹ ë³€í™˜
transformed_data = pre_dataset.map(transform_data)

# ë³€í™˜ëœ ë°ì´í„°ì…‹ ì €ì¥
train_dataset = transformed_data

# í™•ì¸ ì¶œë ¥
train_dataset[0]

from PIL import Image
import requests
from io import BytesIO
from tqdm import tqdm

def convert_to_conversation(sample):
    try:
        # URLì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        url = sample.get('image_url', None)  # ì´ë¯¸ì§€ URL ê°€ì ¸ì˜¤ê¸°
        if not url:
            print("ì´ë¯¸ì§€ URL ì—†ìŒ")
            return None

        res = requests.get(url, stream=True, timeout=10)
        if res.status_code == 200:
            image = Image.open(res.raw)
        else:
            print(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: HTTP {res.status_code}")
            return None

    except Exception as e:
        print(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return None

    try:
        # conversation í˜•ì‹ ë³€í™˜
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": sample['instruction']},
                    {"type": "image", "image": image}
                ]
            },
            {
                "role": "assistant",
                "content": [
                    {"type": "text", "text": sample["output"]}
                ]
            },
        ]
        return {"messages": conversation}
    except Exception as e:
        print(f"ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return None

# ë°ì´í„°ì…‹ ë³€í™˜
converted_dataset = [
    convert_to_conversation(sample) for sample in tqdm(train_dataset, desc="Processing Dataset") if sample is not None
]
converted_dataset = [data for data in converted_dataset if data is not None]  # None ê°’ í•„í„°ë§

len(converted_dataset)

# 3. Before training

FastVisionModel.for_inference(model)
image_url = "https://image.msscdn.net/thumbnails/images/goods_img/20241029/4568758/4568758_17301816059577_big.jpg?w=1200"

print(image_url)

res = requests.get(image_url, stream=True, timeout=10)
image = Image.open(res.raw)
instruction = """
ë‹¹ì‹ ì€ JSON í˜•ì‹ ë°ì´í„°ë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ AIì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ ì˜·ì— ëŒ€í•œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ ì•„ë˜ì˜ í…œí”Œë¦¿ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.


### ì¶œë ¥ í˜•ì‹ ###
{
    "category": "ì˜· ì¢…ë¥˜ (ì˜ˆ: ë¯¼ì†Œë§¤ í‹°ì…”ì¸ , ì²­ë°”ì§€, í›„ë“œ)",
    "gender": "ì°©ìš©ì ì„±ë³„ (ì˜ˆ: ë‚¨, ì—¬, ì •ë³´ ì—†ìŒ)",
    "season": "ê³„ì ˆ ì •ë³´ (ì˜ˆ: SS, FW, ì‚¬ê³„ì ˆ, ì •ë³´ ì—†ìŒ)",
    "color": "ìƒ‰ìƒ (ì˜ˆ: í™”ì´íŠ¸, ë¸”ë™)",
    "material": "ì†Œì¬ (ì˜ˆ: ìš¸, í´ë¦¬, ì •ë³´ ì—†ìŒ)",
    "feature": "íŠ¹ì§• (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´, ì˜ˆ: ìŠ¬ë¦¬ë¸Œë¦¬ìŠ¤, ë°˜íŒ”, ê¸´íŒ”)"
}

### ì…ë ¥ ì´ë¯¸ì§€ ###
[ì•„ë˜ì— ì œê³µëœ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³  JSON ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.]
"""

messages = [
    {"role": "user", "content": [
        {"type": "image"},
        {"type": "text", "text": instruction}
    ]}
]
input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)
inputs = tokenizer(
    image,
    input_text,
    add_special_tokens = False,
    return_tensors = "pt",
).to("cuda")

print("\nBefore training:\n")

text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,
                   use_cache = True, temperature = 1.5, min_p = 0.1)

# 4. Training

FastVisionModel.for_training(model)

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    data_collator = UnslothVisionDataCollator(model, tokenizer),
    train_dataset = converted_dataset,
    args = SFTConfig(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 30,
        learning_rate = 2e-4,
        fp16 = not is_bf16_supported(),
        bf16 = is_bf16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none",
        remove_unused_columns = False,
        dataset_text_field = "",
        dataset_kwargs = {"skip_prepare_dataset": True},
        dataset_num_proc = 4,
        max_seq_length = 2048,
    ),
)

gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

trainer_stats = trainer.train()

used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory         /max_memory*100, 3)
lora_percentage = round(used_memory_for_lora/max_memory*100, 3)
print(f"{trainer_stats.metrics['train_runtime']} seconds used for training.")
print(f"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.")
print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")

# 3. After training

import os
import torch
import requests
from PIL import Image
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

# ëª¨ë¸ ê²½ë¡œ (ë¡œì»¬ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°)
MODEL_PATH = "hateslopacademy/otpensource-vision"

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ë¡œì»¬)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.bfloat16,
    device_map="cuda"
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

FastVisionModel.for_inference(model)

image_url = "https://raw.githubusercontent.com/KingHyeonSeong/otpensource/refs/heads/main/KakaoTalk_20250126_151817195_02.jpg"
res = requests.get(image_url, stream=True, timeout=10)
image = Image.open(res.raw)
instruction = """ You are a fashion expert, and your task is to analyze the clothing in the given image.
Select one or two appropriate category from the list below

category list:
Short Puffer/Heavy Outer
Mustang/Fur
Hood Zip-up
Blouson/MA-1
Leather/Riders Jacket
Trucker Jacket
Suit/Blazer Jacket
Cardigan
Anorak Jacket
Fleece/Teddy Jacket
Training Jacket
Stadium Jacket
Transitional Coat
Winter Single Coat
Winter Double Coat
Other Winter Coats
Long Puffer/Heavy Outer
Padded Vest
Vest
Safari/Hunting Jacket
Nylon/Coach Jacket
Knit/Sweater
Sweatshirt
Hoodie
Shirt/Blouse
PiquÃ©/Collar T-shirt
Long-sleeve T-shirt
Short-sleeve T-shirt
Sleeveless T-shirt
Denim Pants
Training/Jogger Pants
Cotton Pants
Suit Pants/Slacks
Short Pants
Leggings
Jumpsuit/Overall
Mini Dress
Midi Dress
Maxi Dress
Mini Skirt
Midi Skirt
Long Skirt


and include detailed and accurate descriptions for the following features: gender, season, color, material, and extra feature.
Be as specific and precise as possible in your response.
and do not repeat your answer

"""

messages = [
    {"role": "user", "content": [
        {"type": "image"},
        {"type": "text", "text": instruction}
    ]}
]
input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)
inputs = tokenizer(
    image,
    input_text,
    add_special_tokens = False,
    return_tensors = "pt",
).to("cuda")

print("\nAfter training:\n")

text_streamer = TextStreamer(tokenizer, skip_prompt = True)
_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,
                   use_cache = True, temperature = 1.5, min_p = 0.1)


from huggingface_hub.hf_api import HfFolder
HfFolder.save_token("my_hf_token")

import os
import glob
import json
from huggingface_hub import upload_file


# ëª¨ë¸ ì €ì¥ ê²½ë¡œ ë° ì´ë¦„
MODEL_PATH = "lora_model"
HUGGINGFACE_REPO = "hateslopacademy/otpensource-vision-lora"  # ğŸ‘‰ LoRA ëª¨ë¸ ë”°ë¡œ ì—…ë¡œë“œ

# âœ… LoRA ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì €ì¥
model.save_pretrained(MODEL_PATH)  # `.safetensors` í˜•ì‹ìœ¼ë¡œ ì €ì¥
tokenizer.save_pretrained(MODEL_PATH)

# âœ… `.safetensors.index.json` ìë™ ìƒì„± í™•ì¸ ë° ì €ì¥
index_path = os.path.join(MODEL_PATH, "model.safetensors.index.json")

if not os.path.exists(index_path):
    print("âš ï¸ Warning: model.safetensors.index.jsonì´ ì—†ìŠµë‹ˆë‹¤. ìƒì„± ì¤‘...")
    from safetensors.torch import save_file

    # ì €ì¥ëœ safetensors íŒŒì¼ ì°¾ê¸°
    safetensor_files = glob.glob(os.path.join(MODEL_PATH, "*.safetensors"))

    # ì¸ë±ìŠ¤ íŒŒì¼ ìƒì„±
    index_data = {
        "metadata": {"format": "safetensors", "dtype": "float16"},
        "weight_map": {f.split("/")[-1]: f for f in safetensor_files},
    }

    # JSON íŒŒì¼ ì €ì¥
    with open(index_path, "w") as f:
        json.dump(index_data, f)

    print("âœ… model.safetensors.index.json ìƒì„± ì™„ë£Œ!")

# âœ… model.safetensors.index.json ì—…ë¡œë“œ
upload_file(
    path_or_fileobj=index_path,
    path_in_repo="model.safetensors.index.json",
    repo_id=HUGGINGFACE_REPO,
    token=HF_TOKEN,
)

# âœ… LoRA ëª¨ë¸ì„ Hugging Face Hubì— ì—…ë¡œë“œ
model.push_to_hub(
    HUGGINGFACE_REPO,
    token=HF_TOKEN
)

print(f"âœ… LoRA ëª¨ë¸ì´ {HUGGINGFACE_REPO}ì— ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")